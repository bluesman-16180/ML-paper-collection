Notes for "A few useful things to know about machine learning"

. Cost effective to generalize from example, rather than manual programming if we have lots of example.
. Purpose of this article is to communicate knowledge not found in text books,which should be known by practitioners. Tweleve pitfalls are being described in this article.
. While choosing a machine learning approach for an identified problem, we consider three components for our selection of one approach among many other availabe in literature.
. Three components are representation, evaluation, optimization.
. The way we reperesrnt our data to computer and hence the set of classifiers (hypothesis space) plays an important step in choosing one ml approach.
. Evaluation function (scoring function) is needed to pull out bad classifiers from the bad ones.
. If evaluation functions have equivalent scores for several classifiers, one with highere efficieny of learning is choses and this sis where we look for optimization technique for learners.
. It is easy to get hope of false confidance of getting higher accuracy of model on smaller data set, unless a small data aset is kept seperate and unseen.
. For parameter tuning nd testing, cross-validation is the best thing to do.
. We optimize function we dont know using training error to reduce test error, so many a times, local optimum(greedy) might produce better result than global optimum.
. Every learner must embodie some knowledge about problme statement, otherwise its is difficult to beat random guessing.
. Machine learning uses the power of induction, putting little knowledge to take out large knowledge (generalization). Hence, representation of this knowledge helps in our choice in a pproach
. It is good to get 75% accuracy on both training and test set, not 100% on training and 50% on test set. The inconsistancy is a result of overfitting.
. Overfitting has generalization error which can be decomposed into bias and variance. While bias is means learning wrong thing consistenctly, variance is learning rndomness irrespective of real signals.
. Strong false assumptions can be better than weak true ones, because a learner with the latter needs more data to avoid overfitting.
. Overfitting can be avoided by cross-validation, but popular one is regularization term to evalution function that penalize classifier with more structure.
. In case of scarse data availability, statistical significance test such as chi-squared can be done before adding new strucutre.
. It is easy to avoid overfitting(variance) by underfitting (bias) which is wrong thing to do a well. hence, simultaneously avoiding both require learning a perfect classifier.
. Curse of dimentionality can outweight the more number of feature collection for an example problem.
. The main role of theoretical guarantee in machine learning is not as a criterion for practicle decisions, but as a source of understanding and driving force for algorithm design.
. If learner A is better than learner B given infinite data, B is often better than A given finite data.
. 